# Deception-via-sandbagging
Do dialogue RL LMs generate easier questions when you imply the model will have to answer its own question compared to when you imply the generated question will be used by humans.
